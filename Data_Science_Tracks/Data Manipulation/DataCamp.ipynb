{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation with pandas\n",
    "\n",
    "üëã Welcome to your new **workspace**! Here, you can experiment with the data you used in [Data Manipulation with pandas](https://app.datacamp.com/learn/courses/data-manipulation-with-pandas) and practice your newly learned skills with some challenges. You can find out more about DataCamp Workspace [here](https://workspace-docs.datacamp.com/).\n",
    "\n",
    "On average, we expect users to take approximately **30 minutes** to complete the content in this workspace. However, you are free to experiment and practice in it as long as you would like!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get Started\n",
    "Below is a code cell. It is used to execute Python code. The code below imports three packages you used in Data Manipulation with pandas: pandas, NumPy, and Matplotlib. The code also imports data you used in the course as DataFrames using the pandas [`read_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function.\n",
    "\n",
    "üèÉ**To execute the code, click inside the cell to select it and click \"Run\" or the ‚ñ∫ icon. You can also use Shift-Enter to run a selected cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the course packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the four files\n",
    "avocados = pd.read_csv('files/avocado.csv')\n",
    "homelessness = pd.read_csv('files/homelessness.csv')\n",
    "temperatures = pd.read_csv('files/temperatures.csv')\n",
    "walmart = pd.read_csv('files/walmart.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write Code\n",
    "After running the cell above, you have created four pandas DataFrames: `avocado`, `homelessness`, `temperatures`, and `walmart`. \n",
    "‚Äã\n",
    "**Add code** to the code cells below to try one (or more) of the following challenges:\n",
    "‚Äã\n",
    "1. Print the highest weekly sales for each `department` in the `walmart` DataFrame. Limit your results to the top five departments, in descending order. If you're stuck, try reviewing this [video](https://campus.datacamp.com/courses/data-manipulation-with-pandas/aggregating-dataframes?ex=1).\n",
    "2. What was the total `nb_sold` of organic avocados in 2017 in the `avocado` DataFrame? If you're stuck, try reviewing this [video](https://campus.datacamp.com/courses/data-manipulation-with-pandas/slicing-and-indexing-dataframes?ex=6).\n",
    "3. Create a bar plot of the total number of homeless people by region in the `homelessness` DataFrame. Order the bars in descending order. Bonus: create a horizontal bar chart. If you're stuck, try reviewing this [video](https://campus.datacamp.com/courses/data-manipulation-with-pandas/creating-and-visualizing-dataframes?ex=1).\n",
    "4. Create a line plot with two lines representing the temperatures in Toronto and Rome. Make sure to properly label your plot. Bonus: add a legend for the two lines. If you're stuck, try reviewing this [video](https://campus.datacamp.com/courses/data-manipulation-with-pandas/creating-and-visualizing-dataframes?ex=1).\n",
    "‚Äã\n",
    "Be sure to check out the **Answer Key** at the end to see one way to solve each problem. Did you try something similar?\n",
    "‚Äã\n",
    "**Reminder: To execute the code you add to a cell, click inside the cell to select it and click \"Run\" or the ‚ñ∫ icon. You can also use Shift-Enter to run a selected cell.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODULE (1) Transforming DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting a DataFrame\n",
    "When you get a new DataFrame to work with, the first thing you need to do is explore it and see what it contains. There are several useful methods and attributes for this.\n",
    "\n",
    "**.head()** returns the first few rows (the ‚Äúhead‚Äù of the DataFrame). <br>\n",
    "**.info()** shows information on each of the columns, such as the data type and number of missing values. <br>\n",
    "**.shape** returns the number of rows and columns of the DataFrame. <br>\n",
    "**.describe()** calculates a few summary statistics for each column. <br>\n",
    "homelessness is a DataFrame containing estimates of homelessness in each U.S. state in 2018. The individual column is the number of homeless individuals not part of a family with children. The family_members column is the number of homeless individuals part of a family with children. The state_pop column is the state's total population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homelessness.head()\n",
    "homelessness.info()\n",
    "homelessness.shape\n",
    "homelessness.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of a DataFrame\n",
    "To better understand DataFrame objects, it's useful to know that they consist of three components, stored as attributes:\n",
    "\n",
    "**.values:** A two-dimensional NumPy array of values. <br>\n",
    "**.columns:** An index of columns: the column names. <br>\n",
    "**.index:** An index for the rows: either row numbers or row names. <br>\n",
    "You can usually think of indexes as a list of strings or numbers, though the pandas Index data type allows for more sophisticated options. (These will be covered later in the course.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting rows <br>\n",
    "Finding interesting bits of data in a DataFrame is often easier if you change the order of the rows. You can sort the rows by passing a column name to **.sort_values().**\n",
    "\n",
    "In cases where rows have the same value (this is common if you sort on a categorical variable), you may wish to break the ties by sorting on another column. You can sort on multiple columns in this way by passing a list of column names.\n",
    "\n",
    "Sort on ‚Ä¶\tSyntax\n",
    "one column\tdf.sort_values(\"breed\")\n",
    "\n",
    "multiple columns\tdf.sort_values([\"breed\", \"weight_kg\"])\n",
    "\n",
    "By combining .sort_values() with .head(), you can answer questions in the form, \"What are the top cases where‚Ä¶?\".\n",
    "\n",
    ">**Excerise** <br>\n",
    ">1. Create a DataFrame called individuals that contains only the individuals column of homelessness.\n",
    ">Print the head of the result.\n",
    ">homelessness is available and pandas is loaded as pd.\n",
    ">2. Sort homelessness by the number of homeless individuals, from smallest to largest, and save this as homelessness_ind.\n",
    ">Print the head of the sorted DataFrame.\n",
    ">3. Sort homelessness by the number of homeless family_members in descending order, and save this as homelessness_fam.\n",
    ">Print the head of the sorted DataFrame.\n",
    ">4. Sort homelessness first by region (ascending), and then by number of family members (descending). Save this as >homelessness_reg_fam.\n",
    ">Print the head of the sorted DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No 2: Sort homelessness by individuals\n",
    "homelessness_ind = homelessness.sort_values(\"individuals\")\n",
    "\n",
    "# Print the top few rows\n",
    "homelessness_ind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No 3: Sort homelessness by descending family members\n",
    "homelessness_fam = homelessness.sort_values(\"family_members\", ascending = False )\n",
    "\n",
    "# Print the top few rows\n",
    "homelessness_fam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N0 4: Sort homelessness by region, then descending family members\n",
    "homelessness_reg_fam = homelessness.sort_values([\"region\",\"family_members\"], ascending = [True,False])\n",
    "\n",
    "# Print the top few rows\n",
    "homelessness_reg_fam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting columns\n",
    "When working with data, you may not need all of the variables in your dataset. Square brackets ([]) can be used to select only the columns that matter to you in an order that makes sense to you. To select only \"col_a\" of the DataFrame df, use\n",
    "\n",
    "df[\"col_a\"]<br>\n",
    "To select \"col_a\" and \"col_b\" of df, use\n",
    "\n",
    "df[[\"col_a\", \"col_b\"]] <br>\n",
    "homelessness is available and pandas is loaded as pd.\n",
    "\n",
    "**Excerise**<br>\n",
    "1. Create a DataFrame called individuals that contains only the individuals column of homelessness.\n",
    "Print the head of the result.\n",
    "2. Create a DataFrame called state_fam that contains only the state and family_members columns of homelessness, in that order.\n",
    "Print the head of the result.\n",
    "3. Create a DataFrame called ind_state that contains the individuals and state columns of homelessness, in that order.\n",
    "Print the head of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select the individuals column\n",
    "individuals = homelessness['individuals']\n",
    "\n",
    "# Print the head of the result\n",
    "individuals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the state and family_members columns\n",
    "state_fam = homelessness[[\"state\",\"family_members\"]]\n",
    "\n",
    "# Print the head of the result\n",
    "state_fam.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the individuals and state columns, in that order\n",
    "ind_state = homelessness[[\"individuals\",\"state\"]]\n",
    "\n",
    "# Print the head of the result\n",
    "print(ind_state.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting rows\n",
    "A large part of data science is about finding which bits of your dataset are interesting. One of the simplest techniques for this is to find a subset of rows that match some criteria. This is sometimes known as filtering rows or selecting rows.\n",
    "\n",
    "There are many ways to subset a DataFrame, perhaps the most common is to use relational operators to return True or False for each row, then pass that inside square brackets.\n",
    "\n",
    "dogs[dogs[\"height_cm\"] > 60]\n",
    "\n",
    "dogs[dogs[\"color\"] == \"tan\"]\n",
    "\n",
    "You can filter for multiple conditions at once by using the \"bitwise and\" operator, &.\n",
    "\n",
    "dogs[(dogs[\"height_cm\"] > 60) & (dogs[\"color\"] == \"tan\")]\n",
    "homelessness is available and pandas is loaded as pd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Filter homelessness for cases where the number of individuals is greater than ten thousand, assigning to ind_gt_10k. View the printed result.\n",
    "\n",
    "2. Filter homelessness for cases where the USA Census region is \"Mountain\", assigning to mountain_reg. View the printed result.\n",
    "\n",
    "3. Filter homelessness for cases where the number of family_members is less than one thousand and the region is \"Pacific\", assigning to fam_lt_1k_pac. View the printed result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rows where individuals is greater than 10000\n",
    "ind_gt_10k = homelessness[homelessness[\"individuals\"]>10000]\n",
    "\n",
    "# See the result\n",
    "ind_gt_10k.value_counts()\n",
    "ind_gt_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rows where region is Mountain\n",
    "mountain_reg = homelessness[homelessness[\"region\"] == \"Mountain\"]\n",
    "\n",
    "# See the result\n",
    "print(mountain_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rows where family_members is less than 1000 \n",
    "# and region is Pacific\n",
    "fam_lt_1k_pac = homelessness[(homelessness[\"family_members\"]<1000) & (homelessness[\"region\"] == \"Pacific\")]\n",
    "\n",
    "# See the result\n",
    "print(fam_lt_1k_pac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting rows by categorical variables\n",
    "Subsetting data based on a categorical variable often involves using the \"or\" operator (|) to select rows from multiple categories. This can get tedious when you want all states in one of three different regions, for example. Instead, use the .isin() method, which will allow you to tackle this problem by writing one condition instead of three separate ones.\n",
    "\n",
    "colors = [\"brown\", \"black\", \"tan\"]\n",
    "condition = dogs[\"color\"].isin(colors)\n",
    "dogs[condition]\n",
    "homelessness is available and pandas is loaded as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Filter homelessness for cases where the USA census region is \"South Atlantic\" \n",
    "or it is \"Mid-Atlantic\", assigning to south_mid_atlantic. View the printed result.\n",
    "'''\n",
    "# Subset for rows in South Atlantic or Mid-Atlantic regions\n",
    "south_mid_atlantic = homelessness[homelessness['region'].isin([\"South Atlantic\", \"Mid-Atlantic\"])]\n",
    "\n",
    "# See the result\n",
    "south_mid_atlantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Filter homelessness for cases where the USA census state is in the list of Mojave states, canu, \n",
    "assigning to mojave_homelessness. \n",
    "View the printed result.'''\n",
    "# The Mojave Desert states\n",
    "canu = [\"California\", \"Arizona\", \"Nevada\", \"Utah\"]\n",
    "\n",
    "# Filter for rows in the Mojave Desert states\n",
    "mojave_homelessness = homelessness[homelessness[\"state\"].isin(canu)]\n",
    "\n",
    "# See the result\n",
    "print(mojave_homelessness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - NEW COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODULE (3): Slicing and Indexing DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explicit Index**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setting and removing indexes**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Excerise***<br> \n",
    "pandas allows you to designate columns as an index. This enables cleaner code when taking subsets (as well as providing more efficient lookup under some circumstances).\n",
    "\n",
    "In this chapter, you'll be exploring temperatures, a DataFrame of average temperatures in cities around the world. [pandas] is loaded as [pd].\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "100 XP\n",
    "Look at temperatures.<br>\n",
    "    1. Set the index of temperatures to \"city\", assigning to temperatures_ind. <br>\n",
    "    2. Look at temperatures_ind. How is it different from temperatures?<br>\n",
    "    3. Reset the index of temperatures_ind, keeping its contents.<br>\n",
    "    4. Reset the index of temperatures_ind, dropping its contents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Always having an idea of your dataset\n",
    "temperatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at temperatures\n",
    "print(temperatures['city'])\n",
    "#Look at Unique values for city column\n",
    "#print(temperatures['city']).unique()\n",
    "\n",
    "# Index temperatures by city\n",
    "temperatures_ind = temperatures.set_index('city')\n",
    "\n",
    "# Look at temperatures_ind\n",
    "temperatures_ind.head()\n",
    "\n",
    "# Reset the index, keeping its contents\n",
    "print(temperatures_ind.reset_index()).head()\n",
    "\n",
    "# Reset the index, dropping its contents\n",
    "print(temperatures_ind.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Subsetting with .loc[]***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise***<br>\n",
    "The killer feature for indexes is .loc[]: a subsetting method that accepts index values. When you pass it a single argument, it will take a subset of rows.\n",
    "\n",
    "The code for subsetting using .loc[] can be easier to read than standard square bracket subsetting, which can make your code less burdensome to maintain.\n",
    "\n",
    "pandas is loaded as pd. temperatures and temperatures_ind are available; the latter is indexed by city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Instructions***\n",
    "1. Create a list called cities that contains \"Moscow\" and \"Saint Petersburg\".\n",
    "2. Use [] subsetting to filter temperatures for rows where the city column takes a value in the cities list.\n",
    "3. Use .loc[] subsetting to filter temperatures_ind for rows where the city is in the cities list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of cities to subset on\n",
    "cities = [\"Moscow\", \"Saint Petersburg\"]\n",
    "\n",
    "# Subset temperatures using square brackets\n",
    "(temperatures[temperatures[\"city\"].isin(cities)]).head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset temperatures_ind using .loc[]\n",
    "(temperatures_ind.loc[cities]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setting multi-level indexes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise***<br>\n",
    "Indexes can also be made out of multiple columns, forming a multi-level index (sometimes called a hierarchical index). There is a trade-off to using these.\n",
    "\n",
    "The benefit is that multi-level indexes make it more natural to reason about nested categorical variables. For example, in a clinical trial, you might have control and treatment groups. Then each test subject belongs to one or another group, and we can say that a test subject is nested inside the treatment group. Similarly, in the temperature dataset, the city is located in the country, so we can say a city is nested inside the country.\n",
    "\n",
    "The main downside is that the code for manipulating indexes is different from the code for manipulating columns, so you have to learn two syntaxes and keep track of how your data is represented.\n",
    "\n",
    "pandas is loaded as pd. temperatures is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Instructions***<br>\n",
    "100 XPM<br>\n",
    "1. Set the index of temperatures to the \"country\" and \"city\" columns, and assign this to temperatures_ind.\n",
    "2. Specify two country/city pairs to keep: \"Brazil\"/\"Rio De Janeiro\" and \"Pakistan\"/\"Lahore\", assigning to rows_to_keep.\n",
    "3. Print and subset temperatures_ind for rows_to_keep using .loc[]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"1) Set the index of temperatures to the \"country\" and \"city\" columns, and assign this to temperatures_ind\"\"\"\n",
    "# Index temperatures by country & city\n",
    "temperatures_ind = temperatures.set_index([\"country\", \"city\"])\n",
    "\n",
    "\"\"\"2) Specify two country/city pairs to keep: \"Brazil\"/\"Rio De Janeiro\" and \"Pakistan\"/\"Lahore\", assigning to rows_to_keep.\"\"\"\n",
    "# List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore\n",
    "rows_to_keep = [['Brazil', 'Rio De Janeiro'], ['Pakistan', 'Lahore']]\n",
    "\n",
    "\"\"\"3). Print and subset temperatures_ind for rows_to_keep using .loc[].\"\"\"\n",
    "# Subset for rows to keep\n",
    "(temperatures_ind.loc[rows_to_keep]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sorting by index values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise*** <br>\n",
    "Previously, you changed the order of the rows in a DataFrame by calling .sort_values(). It's also useful to be able to sort by elements in the index. For this, you need to use .sort_index().\n",
    "\n",
    "pandas is loaded as pd. temperatures_ind has a multi-level index of country and city, and is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Instructions*** <br>\n",
    "100 XP <br>\n",
    "1. Sort temperatures_ind by the index values.\n",
    "2. Sort temperatures_ind by the index values at the \"city\" level.\n",
    "3. Sort temperatures_ind by ascending country then descending city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort temperatures_ind by index values\n",
    "print(temperatures_ind.sort_index().head())\n",
    "\n",
    "# Sort temperatures_ind by index values at the city level\n",
    "print(temperatures_ind.sort_index(level=['cities']).head())\n",
    "\n",
    "# Sort temperatures_ind by country then descending city\n",
    "print(temperatures_ind.sort_index(level =['country', 'city'], ascending=[True, False]).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Slicing and subsetting with .loc and .iloc**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Slicing index values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise***<br>\n",
    "Slicing index values<br>\n",
    "Slicing lets you select consecutive elements of an object using first:last syntax. DataFrames can be sliced by index values or by row/column number; we'll start with the first case. This involves slicing inside the .loc[] method.\n",
    "\n",
    "Compared to slicing lists, there are a few things to remember.\n",
    "\n",
    ">You can only slice an index if the index is sorted (using .sort_index()).<br>\n",
    ">To slice at the outer level, first and last can be strings.<br>\n",
    ">To slice at inner levels, first and last should be tuples.<br>\n",
    ">If you pass a single slice to .loc[], it will slice the rows.<br>\n",
    "pandas is loaded as pd. temperatures_ind has country and city in the index, and is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Instructions*** <br>\n",
    "100 XP<br>\n",
    "1. Sort the index of temperatures_ind.\n",
    "2. Use slicing with .loc[] to get these subsets:\n",
    "- from Pakistan to Russia.\n",
    "- from Lahore to Moscow. (This will return nonsense.)\n",
    "- from Pakistan, Lahore to Russia, Moscow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the index of temperatures_ind\n",
    "temperatures_srt = temperatures_ind.sort_index()\n",
    "\n",
    "# Subset rows from Pakistan to Russia\n",
    "print(temperatures_srt.loc['Pakistan':'Russia' ])\n",
    "\n",
    "# Try to subset rows from Lahore to Moscow\n",
    "print(temperatures_srt.loc['Lahore':'Moscow'])\n",
    "\n",
    "# Subset rows from Pakistan, Lahore to Russia, Moscow\n",
    "print(temperatures_srt.loc[('Pakistan','Lahore'):('Russia', 'Moscow')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Slicing in both directions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise***<br>\n",
    "You've seen slicing DataFrames by rows and by columns, but since DataFrames are two-dimensional objects, it is often natural to slice both dimensions at once. That is, by passing two arguments to .loc[], you can subset by rows and columns in one go.\n",
    "\n",
    "pandas is loaded as pd. temperatures_srt is indexed by country and city, has a sorted index, and is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Instructions***<br>\n",
    "100 XP\n",
    ">1. Use .loc[] slicing to subset rows from India, Hyderabad to Iraq, Baghdad.\n",
    ">2. Use .loc[] slicing to subset columns from date to avg_temp_c.\n",
    ">3. Slice in both directions at once from Hyderabad to Baghdad, and date to avg_temp_c.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slice rows with code like <br>  \n",
    ">df.loc[(\"a\", \"b\"): (\"c\", \"d\")].<br>\n",
    "\n",
    "Slice columns with code like <bf>   \n",
    ">df.loc[:, \"e\":\"f\"].<br>\n",
    "\n",
    "Slice both ways with code like <br>\n",
    ">df.loc[(\"a\", \"b\"): (\"c\", \"d\"), \"e\":\"f\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures_srt.head()\n",
    "# Subset rows from India, Hyderabad to Iraq, Baghdad\n",
    "print(temperatures_srt.loc[('India', 'Hyderabad'):('Iraq','Baghdad')])\n",
    "\n",
    "# Subset columns from date to avg_tmp_c\n",
    "print(temperatures_srt.loc[:, 'date':'avg_temp_c'])\n",
    "\n",
    "# Subset in both directions at once\n",
    "print(temperatures_srt.loc[('India', 'Hyderabad'):('Iraq','Baghdad'),'date':'avg_temp_c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common ways to subset rows are the ways we‚Äôve previously discussed: using a Boolean condition or by index labels. However, it is also occasionally useful to pass row numbers.\n",
    "\n",
    "This is done using .iloc[], and like .loc[], it can take two arguments to let you subset by rows and columns.\n",
    "\n",
    "pandas is loaded as pd. temperatures (without an index) is available.\n",
    "\n",
    "Use .iloc[] on temperatures to take subsets.\n",
    "\n",
    "Get the 23rd row, 2nd column (index positions 22 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No group keys passed!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\LawalAbbey\\OneDrive\\Documents\\Z-Study Notebooks\\Jupyter Notebook\\Datacamp\\Data Manipulation\\DataCamp.ipynb Cell 60\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/LawalAbbey/OneDrive/Documents/Z-Study%20Notebooks/Jupyter%20Notebook/Datacamp/Data%20Manipulation/DataCamp.ipynb#ch0000065?line=0'>1</a>\u001b[0m temperatures_ind\u001b[39m.\u001b[39;49mpivot_table()\n",
      "File \u001b[1;32mc:\\Users\\LawalAbbey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:8044\u001b[0m, in \u001b[0;36mDataFrame.pivot_table\u001b[1;34m(self, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m   8027\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   8028\u001b[0m \u001b[39m@Appender\u001b[39m(_shared_docs[\u001b[39m\"\u001b[39m\u001b[39mpivot_table\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m   8029\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpivot_table\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   8040\u001b[0m     sort\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   8041\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[0;32m   8042\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreshape\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpivot\u001b[39;00m \u001b[39mimport\u001b[39;00m pivot_table\n\u001b[1;32m-> 8044\u001b[0m     \u001b[39mreturn\u001b[39;00m pivot_table(\n\u001b[0;32m   8045\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   8046\u001b[0m         values\u001b[39m=\u001b[39;49mvalues,\n\u001b[0;32m   8047\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   8048\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   8049\u001b[0m         aggfunc\u001b[39m=\u001b[39;49maggfunc,\n\u001b[0;32m   8050\u001b[0m         fill_value\u001b[39m=\u001b[39;49mfill_value,\n\u001b[0;32m   8051\u001b[0m         margins\u001b[39m=\u001b[39;49mmargins,\n\u001b[0;32m   8052\u001b[0m         dropna\u001b[39m=\u001b[39;49mdropna,\n\u001b[0;32m   8053\u001b[0m         margins_name\u001b[39m=\u001b[39;49mmargins_name,\n\u001b[0;32m   8054\u001b[0m         observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[0;32m   8055\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m   8056\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\LawalAbbey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py:95\u001b[0m, in \u001b[0;36mpivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m     92\u001b[0m     table \u001b[39m=\u001b[39m concat(pieces, keys\u001b[39m=\u001b[39mkeys, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     93\u001b[0m     \u001b[39mreturn\u001b[39;00m table\u001b[39m.\u001b[39m__finalize__(data, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpivot_table\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 95\u001b[0m table \u001b[39m=\u001b[39m __internal_pivot_table(\n\u001b[0;32m     96\u001b[0m     data,\n\u001b[0;32m     97\u001b[0m     values,\n\u001b[0;32m     98\u001b[0m     index,\n\u001b[0;32m     99\u001b[0m     columns,\n\u001b[0;32m    100\u001b[0m     aggfunc,\n\u001b[0;32m    101\u001b[0m     fill_value,\n\u001b[0;32m    102\u001b[0m     margins,\n\u001b[0;32m    103\u001b[0m     dropna,\n\u001b[0;32m    104\u001b[0m     margins_name,\n\u001b[0;32m    105\u001b[0m     observed,\n\u001b[0;32m    106\u001b[0m     sort,\n\u001b[0;32m    107\u001b[0m )\n\u001b[0;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m table\u001b[39m.\u001b[39m__finalize__(data, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpivot_table\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\LawalAbbey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py:164\u001b[0m, in \u001b[0;36m__internal_pivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(values)\n\u001b[1;32m--> 164\u001b[0m grouped \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mgroupby(keys, observed\u001b[39m=\u001b[39;49mobserved, sort\u001b[39m=\u001b[39;49msort)\n\u001b[0;32m    165\u001b[0m agged \u001b[39m=\u001b[39m grouped\u001b[39m.\u001b[39magg(aggfunc)\n\u001b[0;32m    166\u001b[0m \u001b[39mif\u001b[39;00m dropna \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(agged, ABCDataFrame) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(agged\u001b[39m.\u001b[39mcolumns):\n",
      "File \u001b[1;32mc:\\Users\\LawalAbbey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:7718\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   7713\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m   7715\u001b[0m \u001b[39m# https://github.com/python/mypy/issues/7642\u001b[39;00m\n\u001b[0;32m   7716\u001b[0m \u001b[39m# error: Argument \"squeeze\" to \"DataFrameGroupBy\" has incompatible type\u001b[39;00m\n\u001b[0;32m   7717\u001b[0m \u001b[39m# \"Union[bool, NoDefault]\"; expected \"bool\"\u001b[39;00m\n\u001b[1;32m-> 7718\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[0;32m   7719\u001b[0m     obj\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   7720\u001b[0m     keys\u001b[39m=\u001b[39;49mby,\n\u001b[0;32m   7721\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   7722\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   7723\u001b[0m     as_index\u001b[39m=\u001b[39;49mas_index,\n\u001b[0;32m   7724\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m   7725\u001b[0m     group_keys\u001b[39m=\u001b[39;49mgroup_keys,\n\u001b[0;32m   7726\u001b[0m     squeeze\u001b[39m=\u001b[39;49msqueeze,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   7727\u001b[0m     observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[0;32m   7728\u001b[0m     dropna\u001b[39m=\u001b[39;49mdropna,\n\u001b[0;32m   7729\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\LawalAbbey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:882\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[39mif\u001b[39;00m grouper \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    880\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgroupby\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgrouper\u001b[39;00m \u001b[39mimport\u001b[39;00m get_grouper\n\u001b[1;32m--> 882\u001b[0m     grouper, exclusions, obj \u001b[39m=\u001b[39m get_grouper(\n\u001b[0;32m    883\u001b[0m         obj,\n\u001b[0;32m    884\u001b[0m         keys,\n\u001b[0;32m    885\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    886\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m    887\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    888\u001b[0m         observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[0;32m    889\u001b[0m         mutated\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmutated,\n\u001b[0;32m    890\u001b[0m         dropna\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropna,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[0;32m    893\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39m=\u001b[39m obj\n\u001b[0;32m    894\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[1;32mc:\\Users\\LawalAbbey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:910\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    907\u001b[0m     groupings\u001b[39m.\u001b[39mappend(ping)\n\u001b[0;32m    909\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(groupings) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(obj):\n\u001b[1;32m--> 910\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo group keys passed!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    911\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(groupings) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    912\u001b[0m     groupings\u001b[39m.\u001b[39mappend(Grouping(Index([], dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mint\u001b[39m\u001b[39m\"\u001b[39m), np\u001b[39m.\u001b[39marray([], dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintp)))\n",
      "\u001b[1;31mValueError\u001b[0m: No group keys passed!"
     ]
    }
   ],
   "source": [
    "temperatures_ind.pivot_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b8adba4630164ecc3498ece672f77895e3acccfefb26c9155ea3eb82c403f07"
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "4517f65f450b0d20a521ca5b306f7f3feef77e94f7def60b482b88f2033b2b63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
